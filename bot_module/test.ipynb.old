{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D # type: ignore\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # type: ignore\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # type: ignore\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras import callbacks\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lessons.json') as file:\n",
    "    data=json.load(file)\n",
    "\n",
    "sample_sentences=[]\n",
    "sample_labels=[]\n",
    "labels=[]\n",
    "responses=[]\n",
    "\n",
    "for sentence in data['lessons']:\n",
    "    for q in sentence['questions']:\n",
    "        sample_sentences.append(q)\n",
    "        sample_labels.append(sentence['tag'])\n",
    "    responses.append(sentence['responses'])\n",
    "    if sentence['tag'] not in labels:\n",
    "        labels.append(sentence['tag'])\n",
    "\n",
    "\n",
    "num_classes=len(labels)\n",
    "\n",
    "# conversion into model understandable form\n",
    "label_encoder=LabelEncoder()\n",
    "label_encoder.fit(sample_labels)\n",
    "sample_labels=label_encoder.transform(sample_labels)\n",
    "\n",
    "\n",
    "# oov replaces out of the vocabulary words with a special token\n",
    "tokenizer=Tokenizer(num_words=1000,oov_token=\"<OOV>\") # create a vocabulary with limited words\n",
    "tokenizer.fit_on_texts(sample_sentences) # update vocabulary based on sentences\n",
    "word_index=tokenizer.word_index # words indexes in vocabulary\n",
    "sequences=tokenizer.texts_to_sequences(sample_sentences) # transform texts into sequences of integers\n",
    "\n",
    "# transform list of integers into a 2D numpy array (num_samples, num_timesteps)\n",
    "max_padding = 20 # max len of all sequences that others will be 'cut' to\n",
    "padded_sequences=pad_sequences(sequences, truncating='post', maxlen=max_padding)\n",
    "def fit_model(training_variable):\n",
    "    # neural network\n",
    "    model=Sequential()\n",
    "    max_input = 1000\n",
    "    emb_dim = 16\n",
    "    model.add(Embedding(max_input, emb_dim, input_length=max_padding)) # turn positive indexes into a dense vectors of fixed size\n",
    "    model.add(GlobalAveragePooling1D()) # create fixed-length vector for each example by averaging on the sequence dim\n",
    "    model.add(Dense(16, activation='relu')) # implement the activation operation on the input and give an output of length 16\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax')) # output layer with a result of length of labels (categories)\n",
    "\n",
    "    # stochastic gradient descent for large models (adam), loss function for more than two labels\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # model.summary()\n",
    "\n",
    "    # training the model\n",
    "    es = callbacks.EarlyStopping(monitor=\"loss\", mode=\"min\", min_delta=0.05, verbose=1, patience=20, baseline=None, start_from_epoch=200)\n",
    "    \n",
    "    # mc =  callbacks.ModelCheckpoint('best_model.h5', monitor='loss', verbose=0, save_best_only=True) # this actually gives worse results\n",
    "    # num_epochs = 500\n",
    "    hist=model.fit(padded_sequences, np.array(sample_labels), epochs=training_variable, verbose=0, callbacks=[es])\n",
    "    v=(es.stopped_epoch if es.stopped_epoch>0 else training_variable)\n",
    "    # print(\"epoch: \",v)\n",
    "    # hist=model.fit(padded_sequences, np.array(sample_labels), epochs=training_variable, verbose=0)\n",
    "\n",
    "    pyplot.plot(hist.history['accuracy'], label='train')\n",
    "    pyplot.title(training_variable, pad=-80)\n",
    "    \n",
    "    return v\n",
    "\n",
    "num=100\n",
    "result=0\n",
    "epochs=[]\n",
    "while num>=result:\n",
    "    epochs.append(num)\n",
    "    result=fit_model(num)\n",
    "    # pyplot.show()\n",
    "    if result!=num:\n",
    "        print(\"Yummy! I ate \", result, \" epochs <3\")\n",
    "        break\n",
    "    else:\n",
    "        print(result,\": So hungry, need more epochs :(\")\n",
    "        num+=100\n",
    "\n",
    "# for i in range(len(epochs)):\n",
    "#     plot_no = 220 + (i+1)\n",
    "#     pyplot.subplot(plot_no)\n",
    "#     result=fit_model(epochs[i])\n",
    "\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# momentums = ['sgd', 'rmsprop', 'adagrad', 'adam']\n",
    "# varx = [400,500,600,700]\n",
    "varx=[800,900,1000,2000]\n",
    "list1=[]\n",
    "list2=[]\n",
    "list3=[]\n",
    "list4=[]\n",
    "for j in range(10):\n",
    "    print(j)\n",
    "    for i in range(len(varx)):\n",
    "        # print(\"i: \",i)\n",
    "        # plot_no = 220 + (i+1)\n",
    "        # pyplot.subplot(plot_no)\n",
    "        \n",
    "        epoch=fit_model(varx[i])\n",
    "        if i==0:list1.append(epoch)\n",
    "        if i==1:list2.append(epoch)\n",
    "        if i==2:list3.append(epoch)\n",
    "        if i==3:list4.append(epoch)\n",
    "\n",
    "# print(list1,list2,list3,list4)\n",
    "# pyplot.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=pyplot.plot(list1,label='list1')\n",
    "fig=pyplot.plot(list2,label='list2')\n",
    "fig=pyplot.plot(list3,label='list3')\n",
    "fig=pyplot.plot(list4,label='list4')\n",
    "lab=[list1,list2,list3,list4]\n",
    "for i in range(len(lab)):\n",
    "    print(\"list\"+str(i+1)+\" average: \",sum(lab[i])/len(lab[i]))\n",
    "pyplot.legend()\n",
    "pyplot.show(fig)\n",
    "# for 10 iter:\n",
    "# list1 average:  388.5\n",
    "# list2 average:  424.0\n",
    "# list3 average:  394.0\n",
    "# list4 average:  406.6\n",
    "# for 50 iter 400+\n",
    "# list1 average:  383.9\n",
    "# list2 average:  393.72\n",
    "# list3 average:  401.34\n",
    "# list4 average:  396.02\n",
    "\n",
    "# 800 average:  394.62\n",
    "# 900 average:  392.28\n",
    "# 1000 average:  388.28\n",
    "# 2000 average:  398.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
